{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fdc524-8463-46b3-a042-840e72950eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Africa -> 0, 2103 total items\n",
    "# Asia -> 1, 8852 total items\n",
    "# Europe -> 2, 18117 total items\n",
    "# North America -> 3, 14502 total items\n",
    "# Oceania -> 4, 2296 total items\n",
    "# South America -> 5, 4125 total items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80206cc6-c456-4b58-868a-357370a721e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import sklearn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d5de929-2641-4fdf-a372-9e36e7438fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_leaf_files(directory):\n",
    "    try:\n",
    "        children = os.listdir(directory)\n",
    "        child_dirs = [os.path.join(directory, x) for x in children]\n",
    "        total = 0\n",
    "        total += sum([count_leaf_files(x) for x in child_dirs])\n",
    "        return total\n",
    "    except NotADirectoryError as e:\n",
    "        return 1\n",
    "        \n",
    "class GeoGuessrDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.all_files = [x for x in os.listdir(self.img_dir) if 'jpg' in x]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.all_files[idx]\n",
    "        img = torchvision.io.read_image(os.path.join(self.img_dir, filename))\n",
    "        #img = torchvision.transforms.functional.resize(img, [256, 256]) # may help ???\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        label = torch.tensor(int(filename[0]))\n",
    "        if label >= 1:\n",
    "            label = label - 1\n",
    "        \n",
    "        return img.float(), label.long()\n",
    "            \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "519879f3-2527-4e9b-82e0-0895736878d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc36e47e-f02d-4def-8a51-da6fcec794e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.Normalize(mean=[0.5035, 0.5131, 0.4874], std = [0.2178, 0.2180, 0.2618])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2dff9722-39bd-47d1-965d-155c0ebdea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GeoGuessrDataset('continents', transform=transform)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "384d5420-ab93-4db5-8dcc-9cea37c8b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=8, shuffle=True, num_workers = 0, pin_memory=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=8, shuffle=True, num_workers = 0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c57392-547b-4704-9251-228fd1c93b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0.\n",
    "std = 0.\n",
    "n_pixels = 0\n",
    "\n",
    "for data, _ in tqdm(train_dataloader):\n",
    "    batch_samples = data.size(0)\n",
    "    channels = data.size(1)\n",
    "    pixels_per_batch = batch_samples * data.size(2) * data.size(3)\n",
    "\n",
    "    mean += data.sum(dim=[0, 2, 3])\n",
    "    std += (data ** 2).sum(dim=[0, 2, 3])\n",
    "    n_pixels += pixels_per_batch\n",
    "\n",
    "mean /= n_pixels\n",
    "std = (std / n_pixels - mean ** 2).sqrt()\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Std:\", std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ced7a26-b1aa-43f3-98ed-697d56f659f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(in_channels = in_channels, out_channels=out_channels, kernel_size = 3, padding=1)\n",
    "        self.maxpool = torch.nn.MaxPool2d(2)\n",
    "        self.activ = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.activ(x)\n",
    "        return x\n",
    "        \n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_shape=None):\n",
    "        super().__init__()\n",
    "        if input_shape == None:\n",
    "            raise Exception(\"Input shape missing\")\n",
    "\n",
    "        \n",
    "        self.block1 = ConvBlock(3, 16)\n",
    "        self.block2 = ConvBlock(16, 32)\n",
    "        self.block3 = ConvBlock(32, 64)\n",
    "        self.flatten = torch.nn.Flatten(start_dim=1)\n",
    "        with torch.no_grad():\n",
    "            sample = torch.zeros(1, *input_shape)\n",
    "            x = self.block1(sample)\n",
    "            x = self.block2(x)\n",
    "            x = self.block3(x)\n",
    "            x = self.flatten(x)\n",
    "            self.latent_dim = x.shape[1]\n",
    "        \n",
    "        self.dense1 = torch.nn.Linear(self.latent_dim, out_features=128)\n",
    "        self.dense2 = torch.nn.Linear(in_features=128, out_features=64)\n",
    "        self.dense3 = torch.nn.Linear(in_features=64, out_features=6)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7cc92eb4-73fe-43f7-a7e2-3239112272cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.randn(3, 3, 1536, 662).to(device)\n",
    "resized_sample = torch.randn(3, 3, 256, 256).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4dcf50d2-b141-4eb9-922b-f0cbfea909fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_shape=(3, 256, 256)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9608e7a7-ff40-4650-8068-33241af26dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(resized_sample).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2778080f-fa1a-44e3-bfc9-6dc4b9b8a872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "643fc204-8111-48de-8dcd-9ebdb98e4161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    model.eval()\n",
    "    y_total = []\n",
    "    y_pred_total = []\n",
    "    for X, y in tqdm(test_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        outputs = model(X).float()\n",
    "        y_pred = y_pred_total.append(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "        y = y_total.append(y.cpu().numpy())\n",
    "    return y_total, y_pred_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2fc5309e-6515-42ce-a3dd-2bc1fcb42002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1249/1249 [01:10<00:00, 17.83it/s]\n"
     ]
    }
   ],
   "source": [
    "y, y_pred = test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9deed4eb-e8ab-4d5b-8fce-e85be0a619a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [list(x) for x in y]\n",
    "y_pred = [list(x) for x in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e577b94-20fb-455d-ab28-361e9334d835",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [x for x in [a for a in y]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "895cf65a-692e-4eaf-8aee-5d764a2263ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_good = []\n",
    "for a in y:\n",
    "    y_good.extend(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "71e64882-d529-4357-81e3-d58abf27d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_good = []\n",
    "for a in y_pred:\n",
    "    y_pred_good.extend(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fcad48f7-e685-4ec1-9126-a6bf6519d3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52563075690829"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_good, y_pred_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82defdd2-4a2e-49db-99e2-2cb59b64efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs):\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    loss_hist = []\n",
    "    acc_hist = []\n",
    "    step_hist = []\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    i = 0\n",
    "    for epoch in range(epochs):\n",
    "        for X, y in train_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            i += 1\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X).float()\n",
    "            loss = loss_fn(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i % 200 == 0 and i > 200:\n",
    "                y_pred = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "                y = y.cpu().numpy()\n",
    "                acc = accuracy_score(y, y_pred)\n",
    "                \n",
    "                print(f'Epoch loss: {loss.item() / len(y)}, acc: {acc}')\n",
    "                # print(f'Epoch loss: {loss.item() / len(y)}')\n",
    "                loss_hist.append(loss.item() / len(y))\n",
    "                acc_hist.append(acc)\n",
    "                step_hist.append(i)\n",
    "            \n",
    "    return loss_hist, acc_hist, step_hist\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e94191da-e4af-49e4-bd44-dd57bd053737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss: 0.22432903945446014, acc: 0.25\n",
      "Epoch loss: 0.19439999759197235, acc: 0.25\n",
      "Epoch loss: 0.20679756999015808, acc: 0.25\n",
      "Epoch loss: 0.20227986574172974, acc: 0.375\n",
      "Epoch loss: 0.20169922709465027, acc: 0.25\n",
      "Epoch loss: 0.13633376359939575, acc: 0.5\n",
      "Epoch loss: 0.26577767729759216, acc: 0.125\n",
      "Epoch loss: 0.22809475660324097, acc: 0.375\n",
      "Epoch loss: 0.20120921730995178, acc: 0.125\n",
      "Epoch loss: 0.28322476148605347, acc: 0.25\n",
      "Epoch loss: 0.16155576705932617, acc: 0.5\n",
      "Epoch loss: 0.14739269018173218, acc: 0.375\n",
      "Epoch loss: 0.16490530967712402, acc: 0.5\n",
      "Epoch loss: 0.20785440504550934, acc: 0.125\n",
      "Epoch loss: 0.19996735453605652, acc: 0.375\n",
      "Epoch loss: 0.1592976152896881, acc: 0.5\n",
      "Epoch loss: 0.19464966654777527, acc: 0.375\n",
      "Epoch loss: 0.16238349676132202, acc: 0.25\n",
      "Epoch loss: 0.15046878159046173, acc: 0.375\n",
      "Epoch loss: 0.14811421930789948, acc: 0.625\n",
      "Epoch loss: 0.1004604622721672, acc: 0.625\n",
      "Epoch loss: 0.14436259865760803, acc: 0.375\n",
      "Epoch loss: 0.1948218196630478, acc: 0.25\n",
      "Epoch loss: 0.15131612122058868, acc: 0.625\n",
      "Epoch loss: 0.12582612037658691, acc: 0.625\n",
      "Epoch loss: 0.12262693792581558, acc: 0.625\n",
      "Epoch loss: 0.1475125104188919, acc: 0.625\n",
      "Epoch loss: 0.1290084570646286, acc: 0.625\n",
      "Epoch loss: 0.1888657957315445, acc: 0.625\n",
      "Epoch loss: 0.09772280603647232, acc: 0.75\n",
      "Epoch loss: 0.14324253797531128, acc: 0.375\n",
      "Epoch loss: 0.0957116186618805, acc: 0.75\n",
      "Epoch loss: 0.17423222959041595, acc: 0.375\n",
      "Epoch loss: 0.22940301895141602, acc: 0.25\n",
      "Epoch loss: 0.13765157759189606, acc: 0.5\n",
      "Epoch loss: 0.19876629114151, acc: 0.125\n",
      "Epoch loss: 0.15661023557186127, acc: 0.5\n",
      "Epoch loss: 0.12342698872089386, acc: 0.625\n",
      "Epoch loss: 0.10422191768884659, acc: 0.75\n",
      "Epoch loss: 0.12444229423999786, acc: 0.625\n",
      "Epoch loss: 0.13490352034568787, acc: 0.5\n",
      "Epoch loss: 0.1394672840833664, acc: 0.625\n",
      "Epoch loss: 0.11468731611967087, acc: 0.75\n",
      "Epoch loss: 0.1367192566394806, acc: 0.5\n",
      "Epoch loss: 0.20139005780220032, acc: 0.375\n",
      "Epoch loss: 0.19717192649841309, acc: 0.25\n",
      "Epoch loss: 0.1357431560754776, acc: 0.625\n",
      "Epoch loss: 0.16011162102222443, acc: 0.625\n",
      "Epoch loss: 0.11126022785902023, acc: 0.625\n",
      "Epoch loss: 0.1552111804485321, acc: 0.5\n",
      "Epoch loss: 0.10149987787008286, acc: 0.625\n",
      "Epoch loss: 0.06354335695505142, acc: 0.875\n",
      "Epoch loss: 0.1118478775024414, acc: 0.75\n",
      "Epoch loss: 0.1545199602842331, acc: 0.375\n",
      "Epoch loss: 0.17640367150306702, acc: 0.625\n",
      "Epoch loss: 0.058742985129356384, acc: 0.875\n",
      "Epoch loss: 0.07342592626810074, acc: 0.75\n",
      "Epoch loss: 0.060361526906490326, acc: 0.875\n",
      "Epoch loss: 0.11703310906887054, acc: 0.75\n",
      "Epoch loss: 0.17744645476341248, acc: 0.375\n",
      "Epoch loss: 0.11872602999210358, acc: 0.5\n",
      "Epoch loss: 0.15350057184696198, acc: 0.625\n",
      "Epoch loss: 0.20336385071277618, acc: 0.375\n",
      "Epoch loss: 0.17147208750247955, acc: 0.25\n",
      "Epoch loss: 0.11518552899360657, acc: 0.625\n",
      "Epoch loss: 0.21865765750408173, acc: 0.375\n",
      "Epoch loss: 0.13946424424648285, acc: 0.375\n",
      "Epoch loss: 0.0745299831032753, acc: 0.75\n",
      "Epoch loss: 0.18565376102924347, acc: 0.625\n",
      "Epoch loss: 0.1285402923822403, acc: 0.625\n",
      "Epoch loss: 0.09649990499019623, acc: 0.75\n",
      "Epoch loss: 0.12246307730674744, acc: 0.625\n",
      "Epoch loss: 0.059376999735832214, acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "metrics = train(model, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a1fa150-17ef-4881-bcb8-c1cddaefa34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e26122c-338f-4808-a833-c98be6566aba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13db59a3-63f2-4fee-9d4c-2dbc76aea320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "img = dataset[1]\n",
    "print(img)\n",
    "\n",
    "# Dacă e un tuple (imagine, etichetă), extrage doar imaginea:\n",
    "if isinstance(img, (tuple, list)):\n",
    "    img = img[0]\n",
    "\n",
    "# Dacă imaginea e un tensor PyTorch, convertește-l\n",
    "if isinstance(img, torch.Tensor):\n",
    "    img = img.permute(1, 2, 0)  # (C, H, W) -> (H, W, C)\n",
    "    img = img.numpy()\n",
    "\n",
    "# Dacă e nevoie, ajustează forma (opțional, dacă știi forma corectă)\n",
    "# img = img.reshape((662, 1536, 3))  # doar dacă e o imagine plată\n",
    "\n",
    "# Afișează imaginea\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7574243,
     "sourceId": 12037160,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
